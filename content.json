{"meta":{"title":"婺月","subtitle":"","description":"","author":"花无缺","url":"https://hua-wu-que-code.github.io/hexo","root":"/hexo/"},"pages":[{"title":"标题","date":"2024-07-28T06:38:22.598Z","updated":"2024-07-28T06:38:22.598Z","comments":false,"path":"about/index.html","permalink":"https://hua-wu-que-code.github.io/hexo/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Pytorch学习","slug":"Pytorch学习","date":"2024-07-27T13:45:06.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/27/Pytorch学习/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/27/Pytorch%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"pytorch 加载数据Dataset提供一种方式去获取数据及其 label 如何获取每一个数据及其 label 告诉我们总共有多少数据 Dataloader为后面的网络提供不同的数据形式","categories":[{"name":"学习","slug":"学习","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"SoftMax回归","slug":"SoftMax","date":"2024-07-23T23:26:23.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/23/SoftMax/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/23/SoftMax/","excerpt":"","text":"回归可以用于预测多少的问题。比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。 事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”： 某个电子邮件是否属于垃圾邮件文件夹？ 某个用户可能注册或不注册订阅服务？ 某个图像描绘的是驴、狗、猫、还是鸡？ 某人接下来最有可能看哪部电影？ 通常，机器学习实践者用分类这个词来描述两个有微妙差别的问题： 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 我们希望得到“软性”类别，即得到属于每个类别的概率。这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。 分类问题我们从一个图像分类问题开始。假设每次输入是一个的灰度图像。我们可以用一个标量表示每个像素值，每个图像对应四个特征。此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。 接下来，我们要选择如何表示标签。我们有两个明显的选择：最直接的想法是选择，其中整数分别代表狗猫鸡。这是在计算机上存储此类信息的有效方法。如果类别间有一些自然顺序，比如说我们试图预测婴儿儿童青少年青年人中年人老年人，那么将这个问题转变为回归问题，并且保留这种格式是有意义的。 但是一般的分类问题并不与类别之间的自然顺序有关。幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：独热编码（one-hot encoding）。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。在我们的例子中，标签将是一个三维向量，其中对应于“猫”、对应于“鸡”、对应于“狗”： 网络架构为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function）。每个输出对应于它自己的仿射函数。在我们的例子中，由于我们有4个特征和3个可能的输出类别，我们将需要12个标量来表示权重（带下标的），3个标量来表示偏置（带下标的）。下面我们为每个输入计算三个未规范化的预测（logit）：、和。 我们可以用神经网络图 :numref:fig_softmaxreg来描述这个计算过程。与线性回归一样，softmax回归也是一个单层神经网络。由于计算每个输出、和取决于所有输入、、和，所以softmax回归的输出层也是全连接层。 为了更简洁地表达模型，我们仍然使用线性代数符号。通过向量形式表达为，这是一种更适合数学和编写代码的形式。由此，我们已经将所有权重放到一个矩阵中。对于给定数据样本的特征，我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置得到的。 全连接层的参数开销正如我们将在后续章节中看到的，在深度学习中，全连接层无处不在。然而，顾名思义，全连接层是“完全”连接的，可能有很多可学习的参数。具体来说，对于任何具有个输入和个输出的全连接层，参数开销为，这个数字在实践中可能高得令人望而却步。幸运的是，将个输入转换为个输出的成本可以减少到，其中超参数可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性。 softmax运算现在我们将优化参数以最大化观测数据的概率。为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。 我们希望模型的输出可以视为属于类的概率，然后选择具有最大输出值的类别作为我们的预测。例如，如果、和分别为0.1、0.8和0.1，那么我们预测的类别是2，在我们的例子中代表“鸡”。 然而我们能否将未规范化的预测直接视作我们感兴趣的输出呢？答案是否定的。因为将线性层的输出直接视为概率时存在一些问题：一方面，我们没有限制这些输出数字的总和为1。另一方面，根据输入的不同，它们可以为负值。这些违反了 :numref:sec_prob中所说的概率基本公理。 要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。例如，在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。这个属性叫做校准（calibration）。 社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上发明的softmax函数正是这样做的：softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式： 其中:eqlabel:eq_softmax_y_and_o 这里，对于所有的总有。因此，可以视为一个正确的概率分布。softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率。因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是一个线性模型（linear model）。 小批量样本的矢量化:label:subsec_softmax_vectorization 为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算。假设我们读取了一个批量的样本，其中特征维度（输入数量）为，批量大小为。此外，假设我们在输出中有个类别。那么小批量样本的特征为，权重为，偏置为。softmax回归的矢量计算表达式为： :eqlabel:eq_minibatch_softmax_reg 相对于一次处理一个样本，小批量样本的矢量化加快了和的矩阵-向量乘法。由于中的每一行代表一个数据样本，那么softmax运算可以按行（rowwise）执行：对于的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。在 :eqref:eq_minibatch_softmax_reg中，的求和会使用广播机制，小批量的未规范化预测和输出概率都是形状为的矩阵。 损失函数接下来，我们需要一个损失函数来度量预测的效果。我们将使用最大似然估计，这与在线性回归（ :numref:subsec_normal_distribution_and_squared_loss）中的方法相同。 对数似然softmax函数给出了一个向量，我们可以将其视为“对给定任意输入的每个类的条件概率”。例如，=猫。假设整个数据集具有个样本，其中索引的样本由特征向量和独热标签向量组成。我们可以将估计值与实际值进行比较： 根据最大似然估计，我们最大化，相当于最小化负对数似然： 其中，对于任何标签和模型预测，损失函数为： :eqlabel:eq_l_cross_entropy 在本节稍后的内容会讲到， :eqref:eq_l_cross_entropy中的损失函数通常被称为交叉熵损失（cross-entropy loss）。由于是一个长度为的独热编码向量，所以除了一个项以外的所有项都消失了。由于所有都是预测的概率，所以它们的对数永远不会大于。因此，如果正确地预测实际标签，即如果实际标签，则损失函数不能进一步最小化。注意，这往往是不可能的。例如，数据集中可能存在标签噪声（比如某些样本可能被误标），或输入特征没有足够的信息来完美地对每一个样本分类。 softmax及其导数:label:subsec_softmax_and_derivatives 由于softmax和相关的损失函数很常见，因此我们需要更好地理解它的计算方式。将 :eqref:eq_softmax_y_and_o代入损失 :eqref:eq_l_cross_entropy中。利用softmax的定义，我们得到： 考虑相对于任何未规范化的预测的导数，我们得到： 换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。从这个意义上讲，这与我们在回归中看到的非常相似，其中梯度是观测值和估计值之间的差异。这不是巧合，在任何指数族分布模型中（参见本书附录中关于数学分布的一节），对数似然的梯度正是由此得出的。这使梯度计算在实践中变得容易很多。 交叉熵损失现在让我们考虑整个结果分布的情况，即观察到的不仅仅是一个结果。对于标签，我们可以使用与以前相同的表示形式。唯一的区别是，我们现在用一个概率向量表示，如，而不是仅包含二元项的向量。我们使用 :eqref:eq_l_cross_entropy来定义损失，它是所有标签分布的预期损失值。此损失称为交叉熵损失（cross-entropy loss），它是分类问题最常用的损失之一。本节我们将通过介绍信息论基础来理解交叉熵损失。如果想了解更多信息论的细节，请进一步参考本书附录中关于信息论的一节。 信息论基础:label:subsec_info_theory_basics 信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。 熵信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布的熵（entropy）。可以通过以下方程得到： :eqlabel:eq_softmax_reg_entropy 信息论的基本定理之一指出，为了对从分布中随机抽取的数据进行编码，我们至少需要“纳特（nat）”对其进行编码。“纳特”相当于比特（bit），但是对数底为而不是2。因此，一个纳特是比特。 信息量压缩与预测有什么关系呢？想象一下，我们有一个要压缩的数据流。如果我们很容易预测下一个数据，那么这个数据就很容易压缩。为什么呢？举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。由于它们总是相同的，我们总是知道下一个数据是什么。所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。 但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到”惊异”。克劳德·香农决定用信息量来量化这种惊异程度。在观察一个事件时，并赋予它（主观）概率。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。在 :eqref:eq_softmax_reg_entropy中定义的熵，是当分配的概率真正匹配数据生成过程时的信息量的期望。 重新审视交叉熵如果把熵想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？交叉熵从到，记为。我们可以把交叉熵想象为“主观概率为的观察者在看到根据概率生成的数据时的预期惊异”。当时，交叉熵达到最低。在这种情况下，从到的交叉熵是。 简而言之，我们可以从两方面来考虑交叉熵分类目标：（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。 模型预测和评估在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。通常我们使用预测概率最高的类别作为输出类别。如果预测与实际类别（标签）一致，则预测是正确的。在接下来的实验中，我们将使用精度（accuracy）来评估模型的性能。精度等于正确预测数与预测总数之间的比率。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"感知机","slug":"感知机","date":"2024-07-23T14:57:35.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/23/感知机/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/23/%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"","text":"","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"AlexNet 论文精读","slug":"AlexNet 论文精读","date":"2024-07-22T20:50:49.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/22/AlexNet 论文精读/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/22/AlexNet%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","excerpt":"","text":"论文题目：ImageNet Classification with Deep Convolutional Neural Networks一、动机传统的对象识别任务通常依赖于图像数量只有几万张的比较小的数据集（例如 NORB、Caltech-101/256和CIFAR-10/100），对于简单的识别任务，这些规模是足够的。但对于更加复杂的真实世界场景，这些数据集就稍显不足。随着像 ImageNet 这样的大规模数据集的出现，一个能够处理这些数据并充分利用其潜力的模型就至关重要了。本文中提出了一个能够有效处理数据量达到数百万张的图像数据模型 CNNs，并且通过介绍如 ReLu 的非线性激活函数和 dropout 正则化方法来提高对象识别任务的性能。 二、数据集本篇论文使用的数据集为 ImageNet，该数据集包括大约 1500 万张照片，照片中的目标被标记为约 22000 种不同的类别。从 2010 年开始，每年都会举办ILSVRC（ImageNet Large-Scale Visual Recognition Challenge）比赛，该比赛会从 ImageNet 中选出一个子集作为比赛的数据集。ILSVRC通常统计两种错误率：Top-1 错误率和 Top-5 错误率。Top-1 错误率：针对一个样本，如果模型预测概率最大的结果为正确结果，则该样本被统计为预测正确。Top-5 错误率：针对一个样本，如果模型预测概率排名前 5 的结果中包含正确结果，则该样本即被统计为预测正确。 三、模型本篇论文中描述的模型是 AlexNet，即一个深度卷积神经网络（CNN），是一个由五个卷积层和三个全连接层组成的深度卷积神经网络。它的输入层接收224x224像素的RGB图像。模型的前两组卷积层使用大尺寸的卷积核和池化[3]操作。后续的三层卷积层使用较小的3x3卷积核，进一步提取图像特征。模型的最后三层是全连接层，分别包含4096个神经元。输出层是一个softmax层，用于ImageNet 数据集的1000个类别进行分类。模型采用双GPU并行计算来加速训练过程。在AlexNet中，为了避免过拟合，采用了几种策略。首先，使用了数据增强技术，如随机裁剪和水平翻转，以增加训练数据的多样性。其次，模型在全连接层中应用了dropout技术，以0.5的概率随机丢弃神经元，防止过拟合。此外，网络还采用了局部响应归一化（LRN），抑制相邻神经元之间的竞争，进一步提高泛化能力。 四、训练细节AlexNet 使用了 mini-batch 梯度下降法 + Momentum梯度下降法，作者发现较小的权重衰减对于模型的训练十分重要，即权重衰减不仅仅是一个正则化方法，同时也减少了模型的训练误差，权重更新的方法如下：其中，i表示当前的迭代次数，v表示momentum，ϵ表示学习率，⟨∂L/∂w∣wi⟩Di是第i批次的目标函数关于w的导数（wi的偏导数）Di的平均值。 五、实验ILSVRC2010比赛冠军方法是Sparse coding，AlexNet与其比较：ILSVRC-2012，AlexNet参加比赛，获得冠军，远超第二名SIFT+FVs： 六、总结本文指出了 ReLu（Rectified Linear Unit）激活函数在训练深度网络时比传统的激活函数（tanh 和 sigmoid）更为有效，更够加速训练过程提高模型的性能；通过LRN、数据增强（随机裁剪和翻转）和 dropout 技术，AlexNet 有效的减少了过拟合，提高了模型的泛化能力。由于当时的局限性，现在的研究发现，LRN并不是提升性能的必要性，批归一化（Batch Normalization，BN）提供了更好的训练稳定性和性能提升；虽然AlexNet使用了数据增强和dropout，但更先进的方法（如数据扩充、混合现实数据增强、自动数据增强策略等）和更强大的正则化技术（如L2正则化、学习率调度、早停等）已被开发和广泛应用。 七、批注非饱和神经元（non-saturating neurons）是指那些不会轻易达到饱和状态的神经元。饱和状态是指神经元的输出接近其最大或最小值，从而导致输出对输入变化不再敏感。这种现象在传统的激活函数（如Sigmoid和Tanh）中很常见，特别是在输入值很大或很小时。为了避免饱和问题，非饱和神经元通常采用一些不会轻易饱和的激活函数。 ↩在卷积神经网络（CNN）中，“通道”（channel）通常指的是卷积层输出的不同特征图（feature map）。例如经过一个卷积层的处理后，输出的特征图数量就是该层的通道数，每一个通道都对应着不同的特征提取或模式检测。假设我们有一个彩色猫的图片（彩色图片通常由三个通道：红 R，绿 G，蓝B，意味着每个像素的颜色信息由这三个通道的值共同决定），我们通过在 CNN中添加了一个卷积层来处理这个图片。这个卷积层会使用多个卷积核（filters），每个卷积核会扫描图像的不同区域，提取特定的特征（比如边缘，纹理等）。如果我们的卷积层使用了 10 个不同的卷积核，那么该卷积层的输出就会有 10 个不同的“特征图”或“通道”。每个通道都表示了图像中的不同特征。例如，第一个通道可能专注于检测图像中的边缘，第二个通道可能专注于检测猫的耳朵形状，第三个通道可能检测毛发的纹理等等。 ↩池化（Pooling）是一种在卷积神经网络（CNN）中常用的操作,其主要目的是对特征图进行降维、减小计算复杂度，并提取特征的主要信息。池化操作通常通过对特征图的局部区域进行汇总来简化特征表示，同时保留重要的空间信息 。 ↩","categories":[{"name":"论文","slug":"论文","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E8%AE%BA%E6%96%87/"}],"tags":[]},{"title":"BERT 论文精读","slug":"BERT 论文精读","date":"2024-07-17T00:08:21.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/17/BERT 论文精读/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/17/BERT%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","excerpt":"","text":"论文题目：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding一、动机传统语言模型在理解语言时往往是单向的，即它们只能从文本的一个方向（要么从左到右，要么从右到左）来理解信息，这种单向性限制了模型对文本整体含义的理解能力；在预训练阶段和微调阶段使用的目标函数和数据分布可能不一致，导致预训练得到的知识不能很好的迁移到下游任务。为了克服这些问题，BERT 通过深度双向 Transformer 结构来同时考虑输入文本的左侧和右侧上下文，生成更加全面和细致的语言表示；并且，采用了统一的框架进行预训练和微调，减少了预训练和微调阶段的差异，使得预训练得到的知识能够更有效的迁移到各种上下游工作[1]。 二、模型BERT 模型基于 Transformer 架构，通过注意力机制捕捉文本中的长距离依赖关系，它包含多层注意力头，能够并行处理信息，实现深度双向的语言表示。 模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。 BERT 提供了简单和复杂两个模型，对应的超参数分别如下： BERTBASE: L = 12, H=768, A = 12, 参数总量 110M BERTLARGE: L = 24, H=1024, A = 16, 参数总量 110M L:网络层数，即Transformer blocks的数量 A:表示Multi-Head Attention中self-Attention的数量 H：filter的尺寸 三、具体步骤1、预训练任务BERT 是一个多任务模型，它的任务由两个自监督任务组成，即 MLP 和 NSP 1、Task 1:Masked Language ModelMasked Language Model(MLM)和核心思想取自Wilson Taylor在1953年发表的一篇论文。所谓MLM是指在训练的时候随机从输入语料中mask掉一些单词，然后通过上下文进行预测该单词。该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。 在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，80%的时候会直接替换为[MASK],10%替换为其他任意单词，10%的时候保留原始Token。 2、Task 2：Next Sentence PredictionNext Sentence Prediction(NSP)的任务是判断句子B是否是句子A的下文。如果是输出”IsNext”，否则输出”NotNext”。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话随机从语料中提取，它们的关系是NotNext。这个关系保存在图4中的[CLS]符号 2、输入表示BERT的输入的编码向量(长度是512)是3各嵌入特征的单位和，如图4，这三个词嵌入的特征是 WordPiece 嵌入：WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个这种的平衡。图4示例中的playing拆成play和ing位置嵌入(Position Embedding):位置嵌入是将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。分割嵌入(Segment Embedding)：用于区分两个句子，例如B是否是A的下文(对话场景，问答场景等)。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1.最后，说明一下图4中的两个特殊符号[CLS]和[SEP]，其中[CLS]表示该特征用于分类模型，对非分类模型，该符合可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。 3、微调在海量语料训练完BERT之后，便可以将其应用到NLP的各个任务中。对应于NSP任务来说，其条件概率表示为P=softmax(CWT),其中C是BERT输出中的[CLS]符号，W是可学习的权值矩阵,对于其他任务来说，我们也可以根据BERT的输出信息做出相应的预测。 四、实验论文在实验部分对 BERT 在 GLUE（General Lanuage Understanding Evaluation），SQuAD v1.1 （Stanford Question Answering Dataset），SQuAD v2.0等数据集上做了 11 项 NLP 任务，均展示出了 BERT 的优越性。 五、总结BERT 的创新之处在于其深度双向预训练的策略，使得模型在理解语言时能够同时考虑左右两侧的上下文信息。BERT 不仅提升了语言表示的质量，还简化了预训练模型到下游任务的迁移流程。 六、参考在 NLP 中，上游任务（Upstream Tasks）：通常是基础的、提供语言理解和处理能力的NLP任务，如分词、词性标注、句法分析，它们为更复杂的NLP应用打下基础；下游任务（Downstream Tasks）：依赖于上游任务输出的更高级的NLP任务，如情感分析、机器翻译、问答系统等，它们利用上游任务的输出来执行特定应用。 ↩","categories":[{"name":"论文","slug":"论文","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E8%AE%BA%E6%96%87/"}],"tags":[]},{"title":"7.15周报","slug":"7.15周报","date":"2024-07-15T22:32:34.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/15/7.15周报/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/15/7.15%E5%91%A8%E6%8A%A5/","excerpt":"","text":"Attention is all your need尽管 attention 机制由来已久，但真正令其声名大噪的是 google 2017 年的这篇名为《attention is all your need》的论文。 假设我们想用机器翻译的手段将下面这句话翻译成中文： “The animal didn’t cross the street because it was too tired” 当机器读到 “it” 时，“it”代表 “animal” 还是 “street” 呢？对于人类来讲，这是一个极其简单的问题，但是对于机器或者说算法来讲却十分不容易。 self-Attention 则是处理此类问题的一个解决方案，也是目前看起来一个比较好的方案。当模型处理到 “it” 时，self-Attention 可以将 “it” 和“animal‘联系到一起。 它是怎么做到的呢？ 通俗地讲，当模型处理一句话中某一个位置的单词时，self-Attention 允许它看一看这句话中其他位置的单词，看是否能够找到能够一些线索，有助于更好地表示（或者说编码）这个单词。 如果你对 RNN 比较熟悉的话，我们不妨做一个比较。RNN 通过保存一个隐藏态，将前面词的信息编码后依次往后面传递，达到利用前面词的信息来编码当前词的目的。而 self-Attention 仿佛有个上帝之眼，纵观全局，看看上下文中每个词对当前词的贡献。 下面来看下具体是怎么实现的。 Self-Attention in Detail使用向量如下图所示，一般而言，输入的句子进入模型的第一步是对单词进行 embedding，每个单词对应一个 embedding。对于每个 embedding，我们创建三个向量，Query、Key 和 Value 向量。我们如何来创建三个向量呢？如图，我们假设 embedding 的维度为 4，我们希望得到一个维度为 3 的 Query、Key 和 Value 向量，只需将每个 embedding 乘上一个维度为 4*3 的矩阵即可。这些矩阵就是训练过程中要学习的。 那么，Query、Key 和 Value 向量代表什么呢？他们在 attention 的计算中发挥了什么样的作用呢？ 我们用一个例子来说明： 首先要明确一点，self-attention 其实是在计算每个单词对当前单词的贡献，也就是对每个单词对当前单词的贡献进行打分 score。假设我们现在要计算下图中所有单词对第一个单词 “Thinking” 的打分。那么分分数如何计算呢，只需要将该单词的 Query 向量和待打分单词的 Key 向量做点乘即可。比如，第一个单词对第一个单词的分数为 q1× k1，第二个单词对第一个单词的分数为 q1×k2。 我们现在得到了两个单词对第一个单词的打分（分数是个数字了），然后将其进行 softmax 归一化。需要注意的是，在 BERT 模型中，作者在 softmax 之前将分数除以了 Key 的维度的平方根（据说可以保持梯度稳定）。softmax 得到的是每个单词在 Thinking 这个单词上的贡献的权重。显然，当前单词对其自身的贡献肯定是最大的。 接着就是 Value 向量登场的地方了。将上面的分数分别和 Value 向量相乘，注意这里是对应位置相乘。 最后，将相乘的结果求和，这就得到了 self-attention 层对当前位置单词的输出。对每个单词进行如上操作，就能得到整个句子的 attention 输出了。在实际使用过程中，一般采用矩阵计算使整个过程更加高效。 在开始矩阵计算之前，先回顾总结一下上面的步骤： 创建 Query、Key、Value 向量 计算每个单词在当前单词的分数 将分数归一化后与 Value 相乘 求和 值得注意的是，上面阐述的过程实际上是 Attention 机制的计算流程，对于 self-Attention，Query=Value。 Matrix Calculation of Self-Attention其实矩阵计算就是将上面的向量放在一起，同时参与计算。 首先，将 embedding 向量 pack 成一个矩阵 X。假设我们有一句话有长度为 10，embedding 维度为 4，那么 X 的维度为（10 × 4）. 假设我们设定 Q、K、V 的维度为 3. 第二步我们构造一个维度为（4×3）的权值矩阵。将其与 X 做矩阵乘法，得到一个 10×3 的矩阵，这就能得到 Query 了。依样画葫芦，同样可以得到 Key 和 Value。 最后，将 Query 和 Key 相乘，得到打分，然后经过 softmax，接着乘上 V 的到最终的输出。","categories":[{"name":"周报","slug":"周报","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E5%91%A8%E6%8A%A5/"}],"tags":[]},{"title":"latex数学公式","slug":"latex数学公式","date":"2024-07-11T23:24:35.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/11/latex数学公式/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/11/latex%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/","excerpt":"主要介绍了如何在 markdown 中使用 latex 公式","text":"公式的插入 行中公式1$公式$ 独立公式123$$公式$$ 上下标1$x^{y^z} = (1+e^x)^{-2xy^w}$ 括号和分隔符1$f(x,y,z) = 3y^2z\\left(3+\\dfrac{7x+5}{1+y^2}\\right)$ 分数1$\\frac{a}{b}$ 开方1$\\sqrt[根指数,省略是为 2]{被开方数}$ 被开方数根指数省略是为 省略号1234567891011121314151617181920212223242526272829303132333435这些符号用于表示不同类型的省略或延续。下面是每个符号的解释：1. **$\\cdots$**（水平省略号）：表示在水平方向上的省略。例如： \\[ a_1, a_2, a_3, \\cdots, a_n \\] 表示从$a_1$到$a_n$的数列，其中省略了中间的项。2. **$\\ldots$**（水平省略号）：与$\\cdots$类似，表示在水平方向上的省略。通常在文本中表示省略或未完的部分。例如： \\[ 1, 2, 3, \\ldots, n \\] 表示从1到n的数列。3. **$\\vdots$**（竖直省略号）：表示在竖直方向上的省略。例如： \\[ \\begin{matrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{matrix} \\] 表示从$a_1$到$a_n$的一列数，其中省略了中间的项。4. **$\\ddots$**（对角省略号）：表示在对角方向上的省略，通常用于矩阵中。例如： \\[ \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{pmatrix} \\] 表示一个矩阵，其中省略了对角线上的项。 哥放弃了","categories":[{"name":"学习","slug":"学习","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"7.8周报","slug":"7.8周报","date":"2024-07-08T22:32:34.000Z","updated":"2024-07-28T06:38:22.598Z","comments":true,"path":"2024/07/08/7.8周报/","permalink":"https://hua-wu-que-code.github.io/hexo/2024/07/08/7.8%E5%91%A8%E6%8A%A5/","excerpt":"","text":"1.概率论 采样:给定一个概率分布 p(x),生成满足条件的样本,也叫抽样 直接采样 均匀分布: Xt+1 = (aXt + C ) mod m , 也叫线性同余发生器 离散分布: 将概率分布转为连续的值,然后在进行均匀分布采样 其他连续分布 逆变换采样:同离散分布一样,将原分布进行逆变换,然后进行均匀采样 期望:就是随机变量的均值 对于离散变量: 对于连续随机变量: 大数定律: 样本数量充分大的时候,样本均值和期望充分接近 给定 N 个独立分布的样本 其均值收敛于期望值 也就是:$$\\overline{X}N = \\frac{1}{N} \\sum{i=1}^{N} x^{(i)} \\rightarrow E(x) \\text{ as } N \\rightarrow \\infty$$ 2.线性代数 标量对向量的导数（梯度）: 向量对向量的导数（雅可比矩阵）: 标量对矩阵的导数: 3.信息论 在信息论中,熵用来衡量一个随机事件的不确定性 自信息: 熵越高则随机变量的信息越多 熵越低则随机变量的信息越少 在对分布 q(y) 的符号进行编码时,熵 I(q)也是理论上最优编码长度,这种编码方式称为 交叉熵:是用来衡量两个分布之间的差异 交叉熵是按照概率分布 q 的最优编码对真实分布为p 的信息进行编码的长度. 在给定 q 的情况下,如果 p 和 q 越接近,交叉熵越小 如果 p 和 q越远,交叉熵就越大 4.知识图谱内容补充 关系表示学习:通过学习表示实体之间的关系 大规模知识图谱的稀疏性 大规模知识图谱的多样性 实体多样性:知识图谱中包含的实体类型非常丰富,包括但不限于人物.地点.组织.事件.概念,这些实体可以是具体的(历史人物.地标建筑),也可以是抽象的(科学理论.文化概念) 关系多样性:实体之间通过关系相互连接,这些关系可以是简单的(是.有.属于),也可以是复杂的(影响.导致.参与) 属性多样性:每个实体都可能具有多种属性,这些特征可以是实体的基本特征(出生日期.国籍.职业),也可以是实体的其他信息(成就.作品.影响) 语义多样性:知识图谱中的实体和关系不仅具有形式上的多样性,还具有语义上的多样性,不同实体和关系可能在不同的上下稳重具有不同的含义 结构多样性:知识图谱的结构可以是层次画的,也可以是网络化的. 三元组（Triple）在知识图谱和语义网络中用于表示实体及其关系。一个典型的三元组由三个部分组成：主语（subject）、谓语（predicate）和宾语（object）。它们通常表示为 ((S, P, O))，其中： **主语 (Subject, S) **：表示三元组的主体，通常是一个实体或概念。 **谓语 (Predicate, P) **：表示主语和宾语之间的关系或属性。 **宾语 (Object, O) **：表示关系的目标，可以是另一个实体、概念或属性值。 任务语义模式图（Task Semantic Model, TSM）是用于描述特定任务在语义层次上的模型图。它主要关注任务的语义结构和关系，以便更好地理解和执行任务。任务语义模式图通常包括以下几个要素： 任务节点（Task Nodes）：表示具体的任务或子任务，可以是单独的操作、步骤或活动。 语义关系（Semantic Relationships）：描述任务节点之间的关系，如依赖关系、顺序关系或因果关系。 输入输出（Inputs and Outputs）：每个任务节点的输入和输出，表示任务所需的数据和产生的结果。 约束条件（Constraints）：对任务执行的限制条件，如时间约束、资源约束或逻辑约束。 语义标签（Semantic Labels）：为任务节点和关系添加语义标签，以便明确任务的意义和目的。 深度神经网络（Deep Neural Network, DNN） 是一种人工神经网络，其特征是有多个隐藏层。这些隐藏层通过非线性激活函数连接，使网络能够学习复杂的特征和模式。深度神经网络广泛应用于图像识别、语音识别、自然语言处理等领域。其结构一般包括输入层、多个隐藏层和输出层，各层之间通过权重连接，并通过反向传播算法进行训练。 图谱的拓扑结构信息 描述了图中节点和边的连接关系。在知识图谱中，节点表示实体，边表示实体之间的关系。拓扑结构信息帮助理解图的整体形态、节点的连接模式和局部结构，如度分布、聚类系数、路径长度等。它对于分析图谱的性质、优化查询和提升知识图谱的应用效果至关重要。 过拟合（Overfitting） 是机器学习模型在训练数据上表现良好，但在新数据上表现不佳的一种现象。这通常是因为模型在训练过程中学到了训练数据中的噪声和细节，而不是数据的普遍特征。过拟合可以通过增加数据量、正则化、交叉验证、减少模型复杂度等方法来缓解。 时序知识图谱（Temporal Knowledge Graph） 是在传统知识图谱基础上引入时间维度的扩展。它不仅包含实体及其关系，还记录关系发生的时间信息。这使得知识图谱能够表示动态变化的知识，适用于时间敏感的任务，如事件预测、时间序列分析和历史数据挖掘。 实体共现关系（Entity Co-occurrence Relationship） 是指两个或多个实体在同一上下文中同时出现的关系。这种关系可以用于构建实体之间的连接，例如在文档中同时提到的两个人物可以认为具有某种隐含的关联。实体共现关系在信息检索、推荐系统和知识图谱构建中具有重要作用。 1-shot 学习 是少样本学习的一种特殊形式，其中模型在仅有一个示例的情况下进行学习和分类任务。1-shot 学习的目的是使模型能够快速泛化，从少量的示例中学习并应用于新的任务。元学习技术通常用于实现1-shot 学习. 关系表示学习（Relation Representation Learning） 是指在知识图谱中，将关系映射到向量空间中进行表示。通过这种方法，关系的语义和结构特征可以被捕捉和量化，便于在下游任务中使用，如关系预测、链接预测和图嵌入。常用的方法包括基于矩阵分解、图神经网络和深度学习的方法。 查询语义评分（Query Semantic Scoring） 是指对查询语句的语义相关性进行评分的过程。通过这种评分，可以评估查询和候选结果之间的匹配度，从而提高搜索引擎和问答系统的准确性。通常使用自然语言处理技术，如词向量、深度学习模型和语义解析器来实现。 实体角色编码（Entity Role Encoding） 是在知识图谱和自然语言处理任务中，对实体在特定上下文或关系中的角色进行编码表示的方法。它帮助模型理解实体在不同语境中的功能和意义，提高任务的语义理解能力。例如，在句子 “Alice 给了 Bob 一本书” 中，”Alice” 和 “Bob” 分别被编码为 “给书者” 和 “受书者”。 注意力掩码矩阵（Attention Mask Matrix） 是在注意力机制中用于屏蔽无关或无效信息的矩阵。它通常用于序列到序列模型中，以忽略填充的部分或限制注意力的范围。通过掩码矩阵，模型可以更有效地聚焦于重要信息，提高模型的性能和训练效率。 分离式三元组位置编码（Disentangled Triple Position Encoding） 是一种对知识图谱中三元组进行位置编码的方法。它将三元组中的主语、谓语和宾语的位置进行独立编码，捕捉它们在图谱中的位置信息。这种编码方式有助于提高图神经网络在处理知识图谱时的表示能力和泛化性能。 鲁棒性（Robustness） 是指系统或模型在面对各种不确定性、噪声和干扰时，仍能保持正常运行和性能稳定的能力。在机器学习中，鲁棒性通常指模型在处理新数据或遭遇对抗攻击时，能够保持高效的性能。提高鲁棒性的方法包括正则化、数据增强、对抗训练等。 交互连接机制（Interactive Connectivity Mechanism） 是指在神经网络中，通过不同层或模块之间的交互连接来增强信息传递和表示学习的机制。这种机制可以帮助网络捕捉复杂的特征和关系，提高模型的表现力和泛化能力。常见的方法包括残差连接（ResNet）、密集连接（DenseNet）等。 消息传播机制（Message Passing Mechanism） 是图神经网络中的一种重要方法，通过节点之间传递和更新信息来学习图结构中的表示。节点通过接收来自邻居节点的信息，并结合自身的信息进行更新，从而捕捉图中的结构特征和关系。常见的消息传播算法有 Graph Convolutional Networks (GCNs)、Graph Attention Networks (GATs) 等。","categories":[{"name":"周报","slug":"周报","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E5%91%A8%E6%8A%A5/"}],"tags":[]}],"categories":[{"name":"学习","slug":"学习","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"论文","slug":"论文","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E8%AE%BA%E6%96%87/"},{"name":"周报","slug":"周报","permalink":"https://hua-wu-que-code.github.io/hexo/categories/%E5%91%A8%E6%8A%A5/"}],"tags":[]}