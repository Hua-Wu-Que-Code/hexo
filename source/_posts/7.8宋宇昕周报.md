---
title: 7.8 周报
date: 2024-07-08 22:32:34
tags:
mathjax: true
index_img: /img/zb.jpg
categories:
  - 周报
excerpt: 7.1 日到 7.7 日周报
---

# 1.概率论
1. 采样:给定一个概率分布 p(x),生成满足条件的样本,也叫抽样
	1. 直接采样
		1. 均匀分布: X<sub>t+1</sub> = (aX<sub>t</sub> + C ) mod m , 也叫线性同余发生器 
		2. 离散分布: 将概率分布转为连续的值,然后在进行均匀分布采样
	2. 其他连续分布
		1. 逆变换采样:同离散分布一样,将原分布进行逆变换,然后进行均匀采样
2. 期望:就是随机变量的均值
	1. 对于离散变量:$$E[x] = \sum_{n=1}^{N} \lambda_n P(x_n)$$
	2. 对于连续随机变量:$$E[x] = \int_{\mathbb{R}} x p(x) \, dx$$
3. 大数定律: 样本数量充分大的时候,样本均值和期望充分接近
	1. 给定 N 个独立分布的样本 $$x^{1}, x^{2}, \ldots, x^{N} \sim p(x)$$
	2. 其均值收敛于期望值$$\overline{X}_N = \frac{1}{N}(x^{(1)} + x^{(2)} + \ldots + x^{(N)}) \rightarrow E(x) \text{ as } N \rightarrow \infty$$
	3. 也就是:$$\overline{X}_N = \frac{1}{N} \sum_{i=1}^{N} x^{(i)} \rightarrow E(x) \text{ as } N \rightarrow \infty$$
# 2.线性代数
1. 标量对向量的导数（梯度）: $$ \nabla_{\mathbf{x}} f(\mathbf{x}) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right)^T $$
2. 向量对向量的导数（雅可比矩阵）: $$ J_{\mathbf{f}}(\mathbf{x}) = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} $$
3. 标量对矩阵的导数: $$ \frac{\partial f}{\partial \mathbf{X}} = \begin{bmatrix} \frac{\partial f}{\partial X_{11}} & \cdots & \frac{\partial f}{\partial X_{1n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial f}{\partial X_{m1}} & \cdots & \frac{\partial f}{\partial X_{mn}} \end{bmatrix} $$
# 3.信息论
1. 在信息论中,熵用来衡量一个随机事件的不确定性
	1. 自信息: $$I(x) = -\log(p(x))$$
	2. 熵越高则随机变量的信息越多
	3. 熵越低则随机变量的信息越少
	4. 在对分布 q(y) 的符号进行编码时,熵 I(q)也是理论上最优编码长度,这种编码方式称为
2. 交叉熵:是用来衡量两个分布之间的差异
	1.  $$H(X) = E_X[I(x)] \\
= E_X[-\log p(x)] \\
= -\sum_{x \in X} p(x) \log p(x)$$
	2. 交叉熵是按照概率分布 q 的最优编码对真实分布为p 的信息进行编码的长度.
	3. 在给定 q 的情况下,如果 p 和 q 越接近,交叉熵越小
	4. 如果 p 和 q越远,交叉熵就越大

# 4.知识图谱内容补充
1. 关系表示学习:通过学习表示实体之间的关系
2. 大规模知识图谱的稀疏性
3. 大规模知识图谱的多样性
	1. 实体多样性:知识图谱中包含的实体类型非常丰富,包括但不限于人物.地点.组织.事件.概念,这些实体可以是具体的(历史人物.地标建筑),也可以是抽象的(科学理论.文化概念)
	2. 关系多样性:实体之间通过关系相互连接,这些关系可以是简单的(是.有.属于),也可以是复杂的(影响.导致.参与)
	3. 属性多样性:每个实体都可能具有多种属性,这些特征可以是实体的基本特征(出生日期.国籍.职业),也可以是实体的其他信息(成就.作品.影响)
	4. 语义多样性:知识图谱中的实体和关系不仅具有形式上的多样性,还具有语义上的多样性,不同实体和关系可能在不同的上下稳重具有不同的含义
	5. 结构多样性:知识图谱的结构可以是层次画的,也可以是网络化的. 
4. 三元组（Triple）在知识图谱和语义网络中用于表示实体及其关系。一个典型的三元组由三个部分组成：主语（subject）、谓语（predicate）和宾语（object）。它们通常表示为 \((S, P, O)\)，其中：
	1. **主语 (Subject, S) **：表示三元组的主体，通常是一个实体或概念。
	2. **谓语 (Predicate, P) **：表示主语和宾语之间的关系或属性。
	3. **宾语 (Object, O) **：表示关系的目标，可以是另一个实体、概念或属性值。
5. 任务语义模式图（Task Semantic Model, TSM）是用于描述特定任务在语义层次上的模型图。它主要关注任务的语义结构和关系，以便更好地理解和执行任务。任务语义模式图通常包括以下几个要素：
	1. **任务节点（Task Nodes）**：表示具体的任务或子任务，可以是单独的操作、步骤或活动。
	2. **语义关系（Semantic Relationships）**：描述任务节点之间的关系，如依赖关系、顺序关系或因果关系。
	3. **输入输出（Inputs and Outputs）**：每个任务节点的输入和输出，表示任务所需的数据和产生的结果。
	4. **约束条件（Constraints）**：对任务执行的限制条件，如时间约束、资源约束或逻辑约束。
	5. **语义标签（Semantic Labels）**：为任务节点和关系添加语义标签，以便明确任务的意义和目的。
6. **深度神经网络（Deep Neural Network, DNN）** 是一种人工神经网络，其特征是有多个隐藏层。这些隐藏层通过非线性激活函数连接，使网络能够学习复杂的特征和模式。深度神经网络广泛应用于图像识别、语音识别、自然语言处理等领域。其结构一般包括输入层、多个隐藏层和输出层，各层之间通过权重连接，并通过反向传播算法进行训练。
7. **图谱的拓扑结构信息** 描述了图中节点和边的连接关系。在知识图谱中，节点表示实体，边表示实体之间的关系。拓扑结构信息帮助理解图的整体形态、节点的连接模式和局部结构，如度分布、聚类系数、路径长度等。它对于分析图谱的性质、优化查询和提升知识图谱的应用效果至关重要。
8. **过拟合（Overfitting）** 是机器学习模型在训练数据上表现良好，但在新数据上表现不佳的一种现象。这通常是因为模型在训练过程中学到了训练数据中的噪声和细节，而不是数据的普遍特征。过拟合可以通过增加数据量、正则化、交叉验证、减少模型复杂度等方法来缓解。
9. **时序知识图谱（Temporal Knowledge Graph）** 是在传统知识图谱基础上引入时间维度的扩展。它不仅包含实体及其关系，还记录关系发生的时间信息。这使得知识图谱能够表示动态变化的知识，适用于时间敏感的任务，如事件预测、时间序列分析和历史数据挖掘。
10. **实体共现关系（Entity Co-occurrence Relationship）** 是指两个或多个实体在同一上下文中同时出现的关系。这种关系可以用于构建实体之间的连接，例如在文档中同时提到的两个人物可以认为具有某种隐含的关联。实体共现关系在信息检索、推荐系统和知识图谱构建中具有重要作用。
11. **1-shot 学习** 是少样本学习的一种特殊形式，其中模型在仅有一个示例的情况下进行学习和分类任务。1-shot 学习的目的是使模型能够快速泛化，从少量的示例中学习并应用于新的任务。元学习技术通常用于实现1-shot 学习.
12. **关系表示学习（Relation Representation Learning）** 是指在知识图谱中，将关系映射到向量空间中进行表示。通过这种方法，关系的语义和结构特征可以被捕捉和量化，便于在下游任务中使用，如关系预测、链接预测和图嵌入。常用的方法包括基于矩阵分解、图神经网络和深度学习的方法。
13. **查询语义评分（Query Semantic Scoring）** 是指对查询语句的语义相关性进行评分的过程。通过这种评分，可以评估查询和候选结果之间的匹配度，从而提高搜索引擎和问答系统的准确性。通常使用自然语言处理技术，如词向量、深度学习模型和语义解析器来实现。
14. **实体角色编码（Entity Role Encoding）** 是在知识图谱和自然语言处理任务中，对实体在特定上下文或关系中的角色进行编码表示的方法。它帮助模型理解实体在不同语境中的功能和意义，提高任务的语义理解能力。例如，在句子 "Alice 给了 Bob 一本书" 中，"Alice" 和 "Bob" 分别被编码为 "给书者" 和 "受书者"。
15. **注意力掩码矩阵（Attention Mask Matrix）** 是在注意力机制中用于屏蔽无关或无效信息的矩阵。它通常用于序列到序列模型中，以忽略填充的部分或限制注意力的范围。通过掩码矩阵，模型可以更有效地聚焦于重要信息，提高模型的性能和训练效率。
16. **分离式三元组位置编码（Disentangled Triple Position Encoding）** 是一种对知识图谱中三元组进行位置编码的方法。它将三元组中的主语、谓语和宾语的位置进行独立编码，捕捉它们在图谱中的位置信息。这种编码方式有助于提高图神经网络在处理知识图谱时的表示能力和泛化性能。
17. **鲁棒性（Robustness）** 是指系统或模型在面对各种不确定性、噪声和干扰时，仍能保持正常运行和性能稳定的能力。在机器学习中，鲁棒性通常指模型在处理新数据或遭遇对抗攻击时，能够保持高效的性能。提高鲁棒性的方法包括正则化、数据增强、对抗训练等。
18. **交互连接机制（Interactive Connectivity Mechanism）** 是指在神经网络中，通过不同层或模块之间的交互连接来增强信息传递和表示学习的机制。这种机制可以帮助网络捕捉复杂的特征和关系，提高模型的表现力和泛化能力。常见的方法包括残差连接（ResNet）、密集连接（DenseNet）等。
19. **消息传播机制（Message Passing Mechanism）** 是图神经网络中的一种重要方法，通过节点之间传递和更新信息来学习图结构中的表示。节点通过接收来自邻居节点的信息，并结合自身的信息进行更新，从而捕捉图中的结构特征和关系。常见的消息传播算法有 Graph Convolutional Networks (GCNs)、Graph Attention Networks (GATs) 等。