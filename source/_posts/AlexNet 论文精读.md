---
title: AlexNet 论文精读
mathjax: true
index_img: /img/zb.jpg
categories:
  - 论文
date: 2024-07-22 20:50:49
tags:
  - 
---
# 论文题目：ImageNet Classification with Deep Convolutional Neural Networks

# 一、动机
传统的对象识别任务通常依赖于图像数量只有几万张的比较小的数据集（例如 NORB、Caltech-101/256和CIFAR-10/100），对于简单的识别任务，这些规模是足够的。但对于更加复杂的真实世界场景，这些数据集就稍显不足。随着像 ImageNet 这样的大规模数据集的出现，一个能够处理这些数据并充分利用其潜力的模型就至关重要了。本文中提出了一个能够有效处理数据量达到数百万张的图像数据模型 CNNs，并且通过介绍如 ReLu 的非线性激活函数和 dropout 正则化方法来提高对象识别任务的性能。

# 二、模型
本篇论文中描述的模型是 AlexNet，即一个深度卷积神经网络（CNN），是一个由五个卷积层和三个全连接层组成的深度卷积神经网络。它的输入层接收224x224像素的RGB图像。模型的前两组卷积层使用大尺寸的卷积核和池化[^3]操作。后续的三层卷积层使用较小的3x3卷积核，进一步提取图像特征。模型的最后三层是全连接层，分别包含4096个神经元。输出层是一个softmax层，用于ImageNet 数据集的1000个类别进行分类。模型采用双GPU并行计算来加速训练过程。
在AlexNet中，为了避免过拟合，采用了几种策略。首先，使用了数据增强技术，如随机裁剪和水平翻转，以增加训练数据的多样性。其次，模型在全连接层中应用了dropout技术，以0.5的概率随机丢弃神经元，防止过拟合。此外，网络还采用了局部响应归一化（LRN），抑制相邻神经元之间的竞争，进一步提高泛化能力。
![alt text](img/Longshot-20240728140924.png)

# 三、训练细节
AlexNet 使用了 mini-batch 梯度下降法 + Momentum梯度下降法，作者发现较小的权重衰减对于模型的训练十分重要，即权重衰减不仅仅是一个正则化方法，同时也减少了模型的训练误差，权重更新的方法如下：
![alt text](img/Longshot-20240728141505.png)
其中，i表示当前的迭代次数，v表示momentum，ϵ表示学习率，⟨∂L/∂w∣wi⟩Di是第i批次的目标函数关于w的导数（wi的偏导数）Di的平均值。

# 四、实验
ILSVRC2010比赛冠军方法是Sparse coding，AlexNet与其比较：
![alt text](img/Longshot-20240728141739.png)
ILSVRC-2012，AlexNet参加比赛，获得冠军，远超第二名SIFT+FVs：
![alt text](img/Longshot-20240728141746.png)

# 五、总结
本文指出了 ReLu（Rectified Linear Unit）激活函数在训练深度网络时比传统的激活函数（tanh 和 sigmoid）更为有效，更够加速训练过程提高模型的性能；通过LRN、数据增强（随机裁剪和翻转）和 dropout 技术，AlexNet 有效的减少了过拟合，提高了模型的泛化能力。由于当时的局限性，现在的研究发现，LRN并不是提升性能的必要性，批归一化（Batch Normalization，BN）提供了更好的训练稳定性和性能提升；虽然AlexNet使用了数据增强和dropout，但更先进的方法（如数据扩充、混合现实数据增强、自动数据增强策略等）和更强大的正则化技术（如L2正则化、学习率调度、早停等）已被开发和广泛应用。

# 六、批注
[^1]:非饱和神经元（non-saturating neurons）是指那些不会轻易达到饱和状态的神经元。饱和状态是指神经元的输出接近其最大或最小值，从而导致输出对输入变化不再敏感。这种现象在传统的激活函数（如Sigmoid和Tanh）中很常见，特别是在输入值很大或很小时。为了避免饱和问题，非饱和神经元通常采用一些不会轻易饱和的激活函数。
[^2]:在卷积神经网络（CNN）中，“通道”（channel）通常指的是卷积层输出的不同特征图（feature map）。例如经过一个卷积层的处理后，输出的特征图数量就是该层的通道数，每一个通道都对应着不同的特征提取或模式检测。假设我们有一个彩色猫的图片（彩色图片通常由三个通道：红 R，绿 G，蓝B，意味着每个像素的颜色信息由这三个通道的值共同决定），我们通过在 CNN中添加了一个卷积层来处理这个图片。这个卷积层会使用多个卷积核（filters），每个卷积核会扫描图像的不同区域，提取特定的特征（比如边缘，纹理等）。如果我们的卷积层使用了 10 个不同的卷积核，那么该卷积层的输出就会有 10 个不同的“特征图”或“通道”。每个通道都表示了图像中的不同特征。例如，第一个通道可能专注于检测图像中的边缘，第二个通道可能专注于检测猫的耳朵形状，第三个通道可能检测毛发的纹理等等。
[^3]: 池化（Pooling）是一种在卷积神经网络（CNN）中常用的操作,其主要目的是对特征图进行降维、减小计算复杂度，并提取特征的主要信息。池化操作通常通过对特征图的局部区域进行汇总来简化特征表示，同时保留重要的空间信息 。