---
title: 信息论基础
mathjax: true
index_img: /img/zb.jpg
categories:
  - 深度学习
date: 2024-08-01 15:41:49
tags:
---
信息论（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

# 自信息(最短编码长度)
自信息也叫最短编码长度,用来展示一个事件的信息量,信息量具有可加性,当两个独立事件同时发生,其联合概率是相乘.

用一个例子来解释说明,假设现在我们收到了两个消息:
A:今天太阳从东方升起
B:今晚放学别走,我在小树林等你,么么😘

我们认为事件 A 一定会发生,他包含的信息量非常的少,根本没必要发送消息,而事件 B 的信息量非常丰富,嘿嘿嘿.利用这个例子,我们来细化一下信息量的基本想法:①非常有可能发生的事件信息量非常的少,在极端的情况下,确保能够发生的事件应该没有信息量;②不太可能发生的事件要具有更高的信息量.事件包含的信息量应与其发生的概率负相关.

假设$X$是一个离散型随机变量,它的取值范围是:${x_1,x_2,\dots,x_n}$,那么定义事件$X=x_i$的信息量为:$I(x_i)=-log_2 P(X=x_i)$

如图 1 所示。在概率值P趋向于0时，信息量趋向于正无穷，在概率值P趋向于1时，信息量趋向于0，这个函数能够满足信息量的基本想法，可以用来描述信息量。
![图 1](https://raw.githubusercontent.com/Hua-Wu-Que-Code/picture/main/uPic/0eFdvc.png)

# 熵
上面给出的信息量公式只能处理随机变量的取指定值时的信息量，我们可以用香农熵（简称熵）来对整个概率分布的平均信息量进行描述。具体方法为求上述信息量函数关于概率分布P的期望，这个期望值（即熵）为：
$$
H(X) = -\sum_{i=1}^n P(X=x_i) logP(X=x_i)
$$
让我们计算几个例题来对熵有个更深的了解。
<div style="color:red">例题①:</div>求随机变量X的熵，这个随机变量有8种可能的取值${ x_{1}, x_{2},...,x_{8}}$，且每种取值发生的概率都是相等的，即：$P(X=x_1)=P(X=x_2)=...=P(X=x_8)=\frac{1}{8}$
<div style="color:green">解:</div>$H(X) = -8\times \frac{1}{8}log\frac{1}{8}$

<div style="color:red">例题②:</div>还是例题①中的随机变量X，还是8种可能的取值，但是每种取值发生的概率并不是都相等，而是如下所示：$$P(X=x_1)=P(X=x_2)=...=P(X=x_8)= \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64} $$
<div style="color:green">解:</div>$H(X) = -\frac{1}{2} log\frac{1}{2} -\dots -\frac{1}{64} = 2$

# 相对熵(KL 散度)
假设随机变量的真实概率分布为P(X)，而我们在处理实际问题时使用了一个近似的分布来进行建模。由于我们使用的是而不是真实的P(X)，所以我们在具体化的取值时需要一些附加的信息来抵消分布不同造成的影响。我们需要的平均附加信息量可以使用相对熵，或者叫KL散度（Kullback-Leibler Divergence）来计算，KL散度可以用来衡量两个分布的差异：
$$
D_{KL} (P||Q) = -\sum_{i=1}^n P(x_i) logQ(x_i) - (-\sum_{i=1}^n P(x_i) logP(x_i)) = -\sum_{i=1}^n P(x_i) log\frac{P(x_i)}{Q(x_i)}
$$

下面介绍KL散度的两个性质：

① KL散度不是一个对称量，$D_{KL}(P ||Q) \neq D_{KL}(Q||P)$

② KL散度的值始终非零0，当且仅当P(X)=Q(X)时等号成立

# 交叉熵

终于到了主角交叉熵了，其实交叉熵与刚刚介绍的KL散度关系很密切，让我们把上面的KL散度公式换一种写法：
$$
\begin{align}
D_{KL} (P||Q) &= -\sum_{i=1}^n P(x_i) logQ(x_i) - (-\sum_{i=1}^n P(x_i) logP(x_i)) \\
&= -H(P(X)) - (-\sum_{i=1}^n P(x_i) logP(x_i))
\end{align}
$$
交叉熵H(P,Q)就等于：
$$
H(P,Q) = H(P) + D_{KL} (P||Q) = -\sum_{i=1}^n P(x_i) logP(x_i)
$$
也就是KL散度公式的右半部分（带负号）。

细心的小伙伴可能发现了，如果把看作随机变量的真实分布的话，KL散度左半部分的其实是一个固定值，KL散度的大小变化其实是由右半部分交叉熵来决定的，因为右半部分含有近似分布，我们可以把它看作网络或模型的实时输出，把KL散度或者交叉熵看做真实标签与网络预测结果的差异，所以神经网络的目的就是通过训练使近似分布逼近真实分布。从理论上讲，优化KL散度与优化交叉熵的效果应该是一样的。所以我认为，在深度学习中选择优化交叉熵而非KL散度的原因可能是为了减少一些计算量，交叉熵毕竟比KL散度少一项。

# 交叉熵损失函数(Cross Entropy Loss)
刚刚说到，交叉熵是信息论中的一个概念，它与事件的概率分布密切相关，这也就是为什么神经网络在使用交叉熵损失函数时会先使用函数或者函数将网络的输出转换为概率值。

下面从两个方面讨论交叉熵损失函数：

## 交叉熵损失函数在单标签分类任务中的使用（二分类任务包含在其中）
单标签任务，顾名思义，每个样本只能有一个标签，比如ImageNet图像分类任务，或者MNIST手写数字识别数据集，每张图片只能有一个固定的标签。

对单个样本，假设真实分布为y，网络输出分布为$\hat{y}$，总的类别数为n，则在这种情况下，交叉熵损失函数的计算方法为：
$$
Loss = -\sum_{i=1}^n y_i log\hat{y}_i
$$

用一个例子来说明，在手写数字识别任务中，如果样本是数字“5”，那么真实分布应该为：[ 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ]，
如果网络输出的分布为：[ 0.1, 0.1, 0, 0, 0, 0.7, 0, 0.1, 0, 0 ]，那n则应为10，那么计算损失函数得：
$$
Loss = -0 \times log0.1 - \dots -1\times log0.7 \dots \approx 0.3567
$$

如果网络输出的分布为：[ 0.2, 0.3, 0.1, 0, 0, 0.3, 0.1, 0, 0, 0 ]，那么计算损失函数得：1.2040
上述两种情况对比，第一个分布的损失明显低于第二个分布的损失，说明第一个分布更接近于真实分布，事实也确实是这样。

对一个batch，单标签n分类任务的交叉熵损失函数的计算方法为：
$$
Loss = - \frac{1}{batch\_size} \sum_{j=1}^{batch\_size} \sum_{i=1}^{n} y_{ji} log \hat{y_{ji}}
$$