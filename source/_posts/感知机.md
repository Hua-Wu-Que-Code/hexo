---
title: 多层感知机
mathjax: true
index_img: /img/xx.jpg
categories:
  - 深度学习
date: 2024-07-23 14:57:35
tags:
---
# 多层感知机
## 隐藏层
仿射变换， 它是一种带有偏置项的线性变换。 首先，回想一下softmax回归的模型架构。 该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行softmax操作。 如果我们的标签通过仿射变换后确实与我们的输入数据相关，那么这种方法确实足够了。 但是，仿射变换中的线性是一个很强的假设。

## 线性模型可能会出错
例如，线性意味着单调假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。 有时这是有道理的。 例如，如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。 但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。

然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对体温高于37摄氏度的人来说，温度越高风险越大。 然而，对体温低于37摄氏度的人来说，温度越高风险就越低。 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。

但是，如何对猫和狗的图像进行分类呢？ 增加位置$(13,17)$处像素的强度是否总是增加（或降低）图像描绘狗的似然？ 对线性模型的依赖对应于一个隐含的假设， 即区分猫和狗的唯一要求是评估单个像素的强度。 在一个倒置图像后依然保留类别的世界里，这种方法注定会失败。

与我们前面的例子相比，这里的线性很荒谬， 而且我们难以通过简单的预处理来解决这个问题。 这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。 我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。 在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。

## 在网络中加入隐藏层
我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机（ 图1）。
![图1 一个单隐藏层的多层感知机，具有5个隐藏单元](https://raw.githubusercontent.com/Hua-Wu-Que-Code/picture/main/uPic/raTJAd.png)

这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。

## 从线性到非线性
同之前的章节一样， 我们通过矩阵$X\in \mathbb{R}^{n\times d}$来表示$n$个样本的小批量， 其中每个样本具有$d$个输入特征。 对于具有$h$个隐藏单元的单隐藏层多层感知机， 用$H\in \mathbb{R}^{n \times h}$表示隐藏层的输出， 称为隐藏表示（hidden representations）。 在数学或代码中，也被称为隐藏层变量（hidden-layer variable） 或隐藏变量（hidden variable）。 因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d\times h}$和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{1 \times h}$以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{d\times q}$和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{1 \times q}$。 形式上，我们按如下方式计算单隐藏层多层感知机的输出$\mathbf{O} \in \mathbb{R}^{n\times q}$:
$$
\begin{align}
\mathbf{H} &= XW^{(1)} + b^{(1)} \tag{1} \\
\mathbf{O} &= HW^{(2)} + b^{(2)}.
\end{align}
$$

注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。

我们可以证明这一等价性，即对于任意权重值， 我们只需合并隐藏层，便可产生具有参数$\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)} 和 b=b^{(1)} W^{(2)} + b^{(2)}的等价单层模型:$

$$
\mathbf{ O = (XW^{(1)})W^{(2)} + b^{(2)} = XW^{(1)}W^{(2)} + b^{(1)}W^{(2)} + b^{(2)} = XW + b  } \tag{2}
$$

为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： 在仿射变换之后对每个隐藏单元应用非线性的激活函数（activation function）$\sigma$。 激活函数的输出（例如，$\sigma(\cdot)$）被称为活性值（activations）。 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
$$
\begin{align}
H &= \sigma(XW^{(1)} + b^{(1)}), \tag{3}
O &= HW^{(2)} + b^{(2)}.
\end{align}
$$

由于$X$中的每一行对应于小批量中的一个样本， 出于记号习惯的考量， 我们定义非线性函数$\sigma$也以按行的方式作用于其输入， 即一次计算一个样本。 本文应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。 这意味着在计算每一层的线性部分之后，我们可以计算每个活性值， 而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。

为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$H^{(1)} = \sigma_1(XW^{(1)} + b^{(1)})$和
$H^{(2)} = \sigma_2(H^{(1)}W^{(2)} + b^{(2)})$， 一层叠一层，从而产生更有表达能力的模型。

## 通用近似定理
多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算。 例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。 神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。

而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论。

# 激活函数
激活函数（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。

## ReLU 函数
